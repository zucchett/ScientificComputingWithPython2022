# -*- coding: utf-8 -*-
"""07ex_visualization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MBBjxwb1HII9ALOaBCKJu738kqr0vrMJ

1\. **Spotting correlations**

Load the remote file:

```bash
https://www.dropbox.com/s/aamg1apjhclecka/regression_generated.csv
```

with Pandas and create scatter plots with all possible combinations of the following features:
    
  + features_1
  + features_2
  + features_3
  
Are these features correlated?
"""

!wget https://www.dropbox.com/s/aamg1apjhclecka/regression_generated.csv
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv("regression_generated.csv")
ft1 = data.plot.scatter(x='features_1', y='features_2')
ft2 = data.plot.scatter(x='features_2', y='features_3')
ft3 = data.plot.scatter(x='features_1', y='features_3')

"""2\. **Color-coded scatter plot**

Produce a scatter plot from a dataset with two categories.

* Write a function that generates a 2D dataset consisting of 2 categories. Each category should distribute as a 2D gaussian with a given mean and standard deviation. Set different values of the mean and standard deviation between the two samples.
* Display the dataset in a scatter plot marking the two categories with different marker colors.

An example is given below:
"""

from IPython.display import Image
Image('images/two_categories_scatter_plot.png')

#FIRST GAUSSIAN
mean = (1, 3)
cov = [[1, 0], [0, 4]]
category_1 = np.random.multivariate_normal(mean, cov, 50)
x=[]
y=[]
for el in category_1:
    x.append(el[0])
    y.append(el[1])
plt.scatter(x, y, color = 'yellow')

#SECOND GAUSSIAN
mean = (5, 8)
cov = [[2,0], [0, 2]]
category_2 =np.random.multivariate_normal(mean, cov, 50)

x=[]
y=[]
for el in category_2:
    x.append(el[0])
    y.append(el[1])
plt.scatter(x, y, color = 'green')

plt.show()

"""3\. **Profile plot**

Produce a profile plot from a scatter plot.
* Download the following pickle file:
```bash
wget https://www.dropbox.com/s/3uqleyc3wyz52tr/residuals_261.pkl -P data/
```
* Inspect the dataset, you'll find two variables (features)
* Convert the content to a Pandas Dataframe
* Clean the sample by selecting the entries (rows) with the absolute values of the variable "residual" smaller than 2
* Plot a Seaborn `jointplot` of "residuals" versus "distances", and use seaborn to display a linear regression. 

Comment on the correlation between these variables.

* Create manually (without using seaborn) the profile histogram for the "distance" variable; choose an appropriate binning.
* Obtain 3 numpy arrays:
  * `x`, the array of bin centers of the profile histogram of the "distance" variable
  * `y`, the mean values of the "residuals", estimated in slices (bins) of "distance"
  * `err_y`, the standard deviation of the of the "residuals", estimated in slices (bins) of "distance"
* Plot the profile plot on top of the scatter plot
"""

!wget https://www.dropbox.com/s/3uqleyc3wyz52tr/residuals_261.pkl -P data/

import pickle
import seaborn as sns

data="data/residuals_261.pkl"
df = pd.read_pickle(data)

"""4\. **Kernel Density Estimate**

Produce a KDE for a given distribution (by hand, not using seaborn):

* Fill a numpy array `x` of length N (with $N=\mathcal{O}(100)$) with a variable normally distributed, with a given mean and standard deviation
* Fill an histogram in pyplot taking proper care of the aesthetic:
   * use a meaningful number of bins
   * set a proper y axis label
   * set proper value of y axis major ticks labels (e.g. you want to display only integer labels)
   * display the histograms as data points with errors (the error being the poisson uncertainty)
* For every element of `x`, create a gaussian with the mean corresponding to the element value and the standard deviation as a parameter that can be tuned. The standard deviation default value should be:
$$ 1.06 * x.std() * x.size ^{-\frac{1}{5}} $$
you can use the scipy function `stats.norm()` for that.
* In a separate plot (to be placed beside the original histogram), plot all the gaussian functions so obtained
* Sum (with `np.sum()`) all the gaussian functions and normalize the result such that the integral matches the integral of the original histogram. For that you could use the `scipy.integrate.trapz()` method. Superimpose the normalized sum of all gaussians to the first histogram.

"""

import scipy